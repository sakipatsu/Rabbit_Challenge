{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN_after.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakipatsu/Rabbit_Challenge/blob/main/3_1_simple_RNN_after.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bded9c-9e70-49ed-9cf2-65986dbf6abf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "# sys.path.append('/content/drive/My Drive/DNN_code')\n",
        "sys.path.append('/content/drive/My Drive/StudyAI/Step4')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzGmsHRwO-bi"
      },
      "source": [
        "# simple RNN after\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNSG0aKXO-bk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65f4e8cb-29ed-483f-e154-f6397a9b0431"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iters:0\n",
            "Loss:2.746946998589517\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "73 + 56 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9764996746871312\n",
            "Pred:[0 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "39 + 120 = 7\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.0132442204713885\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "125 + 47 = 255\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9832032811977218\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "57 + 85 = 255\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9710080553169168\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "14 + 39 = 255\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0072268796032113\n",
            "Pred:[1 1 1 0 1 1 0 0]\n",
            "True:[1 1 1 0 0 0 0 0]\n",
            "98 + 126 = 236\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.856988629072925\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "50 + 10 = 44\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.9813838578368188\n",
            "Pred:[0 0 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "81 + 22 = 36\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0839214667468888\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "99 + 17 = 199\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0979495112045528\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "47 + 41 = 127\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.801236822499818\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "60 + 59 = 127\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0740253479490798\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "58 + 127 = 101\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.9369518586584781\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "99 + 43 = 223\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.844025530022595\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "8 + 78 = 118\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.6915784703127823\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "68 + 4 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.8304342362356564\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "97 + 24 = 255\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.7760143127363193\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "40 + 19 = 59\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.8473390118619278\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "5 + 117 = 126\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.6186951690045057\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "8 + 70 = 78\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.9531833979060576\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "19 + 102 = 109\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.8043991155066212\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "65 + 79 = 0\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.7008534311996593\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "105 + 1 = 98\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.6215176268782104\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "104 + 72 = 184\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.8032571802061778\n",
            "Pred:[1 1 0 0 1 1 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "52 + 115 = 207\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.6495526392360481\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "64 + 21 = 87\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.8141045457691358\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "94 + 31 = 97\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.9160767013294295\n",
            "Pred:[1 1 1 1 0 1 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "110 + 43 = 245\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.6566186587276569\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "109 + 77 = 178\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.0443833334613222\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "73 + 127 = 128\n",
            "------------\n",
            "iters:2900\n",
            "Loss:1.058635690638532\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "87 + 108 = 187\n",
            "------------\n",
            "iters:3000\n",
            "Loss:1.0464465014184627\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 1 1 0 0 0 0 1]\n",
            "98 + 127 = 157\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.8724886621398097\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "31 + 25 = 38\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.6283072697634083\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "20 + 51 = 111\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.624738135163409\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "78 + 92 = 162\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.5466743028095551\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "56 + 70 = 62\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.31719794531653017\n",
            "Pred:[0 0 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "7 + 50 = 57\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.4881982987350525\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "62 + 48 = 78\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.415492832987707\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "23 + 65 = 94\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.48892982861379225\n",
            "Pred:[1 0 0 1 0 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "46 + 108 = 146\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.7494922904117005\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[0 0 0 0 1 1 1 1]\n",
            "7 + 8 = 19\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.2587427910163559\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "74 + 50 = 124\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.35261000761942757\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "5 + 76 = 81\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.6661767813435189\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "108 + 59 = 131\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.04917938364796979\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "50 + 72 = 122\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.4910321691427224\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 1 0 0 1 1 0 1]\n",
            "115 + 90 = 173\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.506297336591335\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "13 + 91 = 96\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.8178326547220304\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "93 + 31 = 96\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.09605737925524957\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "88 + 99 = 187\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.5010871315184632\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "56 + 125 = 133\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.5260812493803906\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "127 + 56 = 135\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.691831549552568\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "27 + 119 = 128\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.27007178448722363\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "114 + 50 = 132\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.029895595439653238\n",
            "Pred:[0 0 1 1 1 1 0 1]\n",
            "True:[0 0 1 1 1 1 0 1]\n",
            "9 + 52 = 61\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.1239325065004473\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "3 + 105 = 108\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.3137646587982471\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "101 + 94 = 195\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.39718169013292065\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "59 + 120 = 131\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.015096821133682827\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "82 + 8 = 90\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.14767511204175907\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "101 + 30 = 131\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.39356354208944533\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "31 + 75 = 104\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.22571504287267508\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "126 + 11 = 137\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.12729679012212536\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "15 + 70 = 85\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.04593750015188063\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "66 + 13 = 79\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.013306557208460737\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "75 + 84 = 159\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.015486869013567417\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "69 + 54 = 123\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.015309777282658464\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "72 + 104 = 176\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.03412328978737552\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "41 + 73 = 114\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.016224042201546354\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "36 + 102 = 138\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.039608851114392996\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "10 + 57 = 67\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.09000472254439752\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "78 + 22 = 100\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.09491581759945653\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "47 + 49 = 96\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.028982690988679734\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "73 + 126 = 199\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.04291777282891322\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "23 + 42 = 65\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.00405135658731937\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "51 + 76 = 127\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.010114176573668528\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "76 + 104 = 180\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.04391832922685789\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "46 + 92 = 138\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.08405392763277608\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "7 + 71 = 78\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.010076857880791068\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "64 + 91 = 155\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.059022779708686086\n",
            "Pred:[0 0 1 1 1 0 1 0]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "15 + 43 = 58\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.04120429451363978\n",
            "Pred:[1 1 1 0 0 1 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "116 + 112 = 228\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.007873166671934167\n",
            "Pred:[1 0 0 1 1 0 1 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "104 + 50 = 154\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.004798741474104931\n",
            "Pred:[0 0 1 0 1 0 0 0]\n",
            "True:[0 0 1 0 1 0 0 0]\n",
            "36 + 4 = 40\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0035222144507283696\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "21 + 66 = 87\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.003776696576456959\n",
            "Pred:[1 0 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "61 + 96 = 157\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0046402825691638895\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "0 + 42 = 42\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.010586214077252541\n",
            "Pred:[1 1 0 0 1 0 1 0]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "101 + 101 = 202\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.009555683029214921\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "86 + 114 = 200\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.012638401482569537\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "54 + 69 = 123\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.03784274029164961\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "107 + 55 = 162\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.004893922941548483\n",
            "Pred:[0 1 1 0 1 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "33 + 73 = 106\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.004661584058878214\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "107 + 44 = 151\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0053630421004330865\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "41 + 19 = 60\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.002368304598418699\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "51 + 108 = 159\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.03355665041179029\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "110 + 51 = 161\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.003784495926108219\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "50 + 38 = 88\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.00266324903786125\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "51 + 36 = 87\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.027071515411296952\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "87 + 83 = 170\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.020644711378198514\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[1 1 1 0 1 1 1 1]\n",
            "126 + 113 = 239\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.025045388473596133\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "94 + 47 = 141\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.010415610067680284\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "53 + 47 = 100\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.005045256261568807\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "106 + 26 = 132\n",
            "------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "# W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "# W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "# W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "#         z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        z[:,t+1] = functions.relu(u[:,t+1])\n",
        "#         z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gpWiHb79pVUZ",
        "outputId": "ed7ddf1d-738b-4237-c03b-50074dee0487"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iters:0\n",
            "Loss:0.7709118104634634\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "45 + 81 = 254\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.8682925167066824\n",
            "Pred:[1 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "93 + 30 = 227\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.0818564005132236\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "93 + 123 = 128\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.8738785804375904\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "111 + 27 = 132\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.2252644927955655\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "26 + 83 = 109\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.46576948983663624\n",
            "Pred:[1 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "99 + 7 = 232\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.15299772883375043\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "109 + 78 = 179\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.43267192617417266\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "40 + 96 = 136\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.016021201539440224\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "35 + 82 = 117\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.27941611556794016\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "118 + 109 = 163\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.004107752293503058\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "66 + 11 = 77\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.7644003118797882\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "15 + 126 = 113\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.7600104358272248\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "110 + 63 = 221\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.21761600700649594\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "47 + 33 = 80\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.0026724187352627975\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "41 + 37 = 78\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.8365951401221453\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "82 + 114 = 164\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.768803229787087\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 0 0 0 1 0]\n",
            "30 + 4 = 54\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.2669749119460017\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "44 + 48 = 92\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.5034650599053943\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "60 + 43 = 69\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.7819001477930739\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "102 + 126 = 128\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.9659882103381316\n",
            "Pred:[0 0 0 1 1 0 0 0]\n",
            "True:[0 0 1 0 0 0 1 0]\n",
            "7 + 27 = 24\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.545907014601619\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "119 + 26 = 129\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.10890799700527634\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "53 + 25 = 78\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.000988254724080342\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "87 + 24 = 111\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.34494048808899413\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "77 + 26 = 119\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.39428450492679834\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "59 + 40 = 83\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.0003056882609490748\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "55 + 72 = 127\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.4019460713713034\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "72 + 88 = 160\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.7949389802396687\n",
            "Pred:[1 1 1 0 1 1 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "95 + 49 = 236\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.019980520258453874\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "35 + 9 = 44\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.0005427795600940036\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "67 + 58 = 125\n",
            "------------\n",
            "iters:3100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "3 + 64 = 0\n",
            "------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/My Drive/StudyAI/Step4/common/functions.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iters:3200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "38 + 67 = 0\n",
            "------------\n",
            "iters:3300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "63 + 89 = 0\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "56 + 73 = 0\n",
            "------------\n",
            "iters:3500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "55 + 120 = 0\n",
            "------------\n",
            "iters:3600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "76 + 6 = 0\n",
            "------------\n",
            "iters:3700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "23 + 94 = 0\n",
            "------------\n",
            "iters:3800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "124 + 108 = 0\n",
            "------------\n",
            "iters:3900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "61 + 16 = 0\n",
            "------------\n",
            "iters:4000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "113 + 108 = 0\n",
            "------------\n",
            "iters:4100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "44 + 53 = 0\n",
            "------------\n",
            "iters:4200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "17 + 104 = 0\n",
            "------------\n",
            "iters:4300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "71 + 7 = 0\n",
            "------------\n",
            "iters:4400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "45 + 55 = 0\n",
            "------------\n",
            "iters:4500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "111 + 66 = 0\n",
            "------------\n",
            "iters:4600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "19 + 96 = 0\n",
            "------------\n",
            "iters:4700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "74 + 92 = 0\n",
            "------------\n",
            "iters:4800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "124 + 57 = 0\n",
            "------------\n",
            "iters:4900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 1]\n",
            "56 + 113 = 0\n",
            "------------\n",
            "iters:5000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "45 + 22 = 0\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 1]\n",
            "109 + 100 = 0\n",
            "------------\n",
            "iters:5200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "50 + 10 = 0\n",
            "------------\n",
            "iters:5300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "11 + 76 = 0\n",
            "------------\n",
            "iters:5400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "66 + 120 = 0\n",
            "------------\n",
            "iters:5500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 1 1]\n",
            "87 + 104 = 0\n",
            "------------\n",
            "iters:5600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "8 + 61 = 0\n",
            "------------\n",
            "iters:5700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "112 + 0 = 0\n",
            "------------\n",
            "iters:5800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "98 + 90 = 0\n",
            "------------\n",
            "iters:5900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "77 + 74 = 0\n",
            "------------\n",
            "iters:6000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "48 + 29 = 0\n",
            "------------\n",
            "iters:6100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "11 + 103 = 0\n",
            "------------\n",
            "iters:6200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 0 0 0]\n",
            "39 + 1 = 0\n",
            "------------\n",
            "iters:6300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "39 + 125 = 0\n",
            "------------\n",
            "iters:6400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "126 + 87 = 0\n",
            "------------\n",
            "iters:6500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "85 + 16 = 0\n",
            "------------\n",
            "iters:6600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 0 1]\n",
            "105 + 120 = 0\n",
            "------------\n",
            "iters:6700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "95 + 97 = 0\n",
            "------------\n",
            "iters:6800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "93 + 64 = 0\n",
            "------------\n",
            "iters:6900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "80 + 15 = 0\n",
            "------------\n",
            "iters:7000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "105 + 42 = 0\n",
            "------------\n",
            "iters:7100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "107 + 110 = 0\n",
            "------------\n",
            "iters:7200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "101 + 75 = 0\n",
            "------------\n",
            "iters:7300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "61 + 84 = 0\n",
            "------------\n",
            "iters:7400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "69 + 33 = 0\n",
            "------------\n",
            "iters:7500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "5 + 125 = 0\n",
            "------------\n",
            "iters:7600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "78 + 20 = 0\n",
            "------------\n",
            "iters:7700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "95 + 25 = 0\n",
            "------------\n",
            "iters:7800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "90 + 97 = 0\n",
            "------------\n",
            "iters:7900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "16 + 90 = 0\n",
            "------------\n",
            "iters:8000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "110 + 84 = 0\n",
            "------------\n",
            "iters:8100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "75 + 73 = 0\n",
            "------------\n",
            "iters:8200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "118 + 4 = 0\n",
            "------------\n",
            "iters:8300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "71 + 95 = 0\n",
            "------------\n",
            "iters:8400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "63 + 36 = 0\n",
            "------------\n",
            "iters:8500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "47 + 34 = 0\n",
            "------------\n",
            "iters:8600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 0 0]\n",
            "7 + 25 = 0\n",
            "------------\n",
            "iters:8700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "56 + 97 = 0\n",
            "------------\n",
            "iters:8800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "18 + 64 = 0\n",
            "------------\n",
            "iters:8900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "64 + 68 = 0\n",
            "------------\n",
            "iters:9000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "71 + 116 = 0\n",
            "------------\n",
            "iters:9100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "84 + 0 = 0\n",
            "------------\n",
            "iters:9200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "65 + 95 = 0\n",
            "------------\n",
            "iters:9300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "62 + 44 = 0\n",
            "------------\n",
            "iters:9400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "21 + 35 = 0\n",
            "------------\n",
            "iters:9500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 1 1 1 0 1]\n",
            "126 + 127 = 0\n",
            "------------\n",
            "iters:9600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "62 + 25 = 0\n",
            "------------\n",
            "iters:9700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "72 + 72 = 0\n",
            "------------\n",
            "iters:9800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "104 + 28 = 0\n",
            "------------\n",
            "iters:9900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "78 + 56 = 0\n",
            "------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5QcZ3mnn7dvMz1XjTSSMLrY8gWICWCI1pjLgp2AMWwWJ7ucE3tJ4mTh+JwsZDfJOWTxyS5snD9y27MhCSTGJ3HIhsXmEkgc1mBIsCGJMbG8GGOMZSQZ62JbM9JoNKO59KXq3T+qqqe6u6q7JVVrpqrf55w5011V3f3V1Hy/evv3vd/7iapiGIZhDA659W6AYRiGcWEx4TcMwxgwTPgNwzAGDBN+wzCMAcOE3zAMY8AorHcDopientZLLrlkvZthGIaRGh599NETqrq1l2M3pPBfcskl7Nu3b72bYRiGkRpE5NlejzWrxzAMY8Aw4TcMwxgwTPgNwzAGDBN+wzCMAcOE3zAMY8Aw4TcMwxgwTPgNwzAGDBN+oysPHTzBwdkz690MwzASwoTf6Mp//evHuePBg+vdDMMwEsKE3+jKas2l6rjr3QzDMBKiq/CLyF0iMiMiT8Ts/4CIPOb/PCEijohs9vf9UES+6+8bmBoMqzWHLK1sVnNc6m52zscwBp1eIv5PADfE7VTV31fVq1T1KuA24OuqOhc65Dp//97za2o6OLVU5VW/+RW+efDkejclMeqO4jgm/IaRFboKv6p+A5jrdpzPzcDd59WilDO3XKVSdzl6amW9m5IYFvEbRrZIzOMXkRG8bwZ/HdqswFdE5FERubXL628VkX0ism92djapZl1wHF8gK3VnnVuSHHVXcVzz+A0jKyQ5uPtvgX9usXneqKqvAd4OvE9E3hT3YlW9U1X3qurerVt7Kim9Iak7gfBnQyhVFcdVi/gNI0MkKfw30WLzqOox//cM8AXg6gQ/b0MSRPxZyYKp+Tcyx4TfMDJDIsIvIpPAm4G/DW0bFZHx4DFwPRCZGZQl6r4lUs1IxB+cj0X8hpEduq7AJSJ3A9cC0yJyFPgwUARQ1Tv8w34a+IqqLoVeuh34gogEn/MpVf1yck3fmAQCmRXht4jfMLJHV+FX1Zt7OOYTeGmf4W2HgFeda8PSSuDxZ0f4LeI3jKxhM3cTJmsef70R8WfjfAzDMOFPnKx5/I2I3yZwGUZmMOFPGCdjHn9g8ZjHbxjZwYQ/YeputvL4637Eb8JvGNnBhD9hnIwJf5DVY4O7hpEdTPgTpp61wV3XIn7DyBom/AnjNAZ3s1GrZy2dMxs3MsMwTPgTJ3t5/Da4axhZw4Q/YbKax28ev2FkBxP+hMlcyYbA47c8fsPIDCb8CZO5PH6L+A0jc5jwJ4zl8RuGsdEx4U8YJ2slG9wg4s/G+RiGYcKfOFnz+IOI31VwLeo3jExgwp8wwSBoJSNZPbXQeThqwm8YWcCEP2FqoYhfMyCUtVA2j/n8hpENTPgTJly3vpaBFMh6KOK3zB7DyAYm/AkTFscsTOIKn4/l8htGNjDhT5iwOGZhgDf8rcUyewwjG3QVfhG5S0RmROSJmP3XishpEXnM//lQaN8NIrJfRA6IyAeTbPhGpSniz4Dwh60e8/gNIxv0EvF/ArihyzH/qKpX+T+3A4hIHvgY8HbgSuBmEbnyfBqbBsLiWMlAhc6aG474TfgNIwt0FX5V/QYwdw7vfTVwQFUPqWoVuAe48RzeJ1VkOeK3dXcNIxsk5fG/TkS+IyJfEpGX+9t2AEdCxxz1t0UiIreKyD4R2Tc7O5tQsy484ayeLJRtqDVl9aT/fAzDSEb4/x9wsaq+Cvhj4G/O5U1U9U5V3auqe7du3ZpAs9aHrGX1WB6/YWSP8xZ+VV1Q1TP+4/uAoohMA8eAXaFDd/rbMo2TNavHtTx+w8ga5y38IvIiERH/8dX+e54EHgGuEJE9IlICbgLuPd/P2+hkz+O3iN8wskah2wEicjdwLTAtIkeBDwNFAFW9A3gX8EsiUgdWgJvUq1VQF5H3A/cDeeAuVf1eX85iA5HtPH4TfsPIAl2FX1Vv7rL/o8BHY/bdB9x3bk1LJ3VXKeaFmqOZ8PjDVo9jg7uGkQls5m7COK5LuZgHshHxh60eS+c0jGxgwp8wdVcZKXlfpLIwgatqM3cNI3OY8CeM4yojpSxF/JbVYxhZw4Q/YequUvaFPwsTuOqukhPvsUX8hpENTPgTpu64axF/BgZ3a47LsD9mYRG/YWQDE/6EcVyl7Hv82bB6tCH8ltVjGNnAhD9h6q5SygvFvGRC+GuuMlzw/k0s4jeMbGDCnzCOq+RzQimfy4Tw10NWj3n8hpENTPgTpu4qhVyOUiGXCY+/7ihDgcdvefyGkQlM+BOmEfEXshHxe4O73r+JRfyGkQ1M+BOm7roUfOHPQjpnLTQT2Tx+w8gGJvwJ4zhZ8/gtq8cwsoYJf8LUXaWQF0qFfDYifkcbVo9F/IaRDUz4E6bJ48/C4K7rMlywrB7DyBIm/AkTZPUMFXJUM1CkrSmrx4TfMDKBCX/COK5SyIkv/OmP+C2rxzCyhwl/wtRdl3zeH9zNhNWzNrhrefyGkQ1M+BMmiPizkMfvuorjKqV8DhHL6jGMrNBV+EXkLhGZEZEnYva/W0QeF5HvishDIvKq0L4f+tsfE5F9STZ8o1J3lbw/czftWT01X+hLhRyFnJjHbxgZoZeI/xPADR32PwO8WVVfAfwWcGfL/utU9SpV3XtuTUwPrquo4kX8GcjjD6ydQk7I58Q8fsPICL0stv4NEbmkw/6HQk8fBnaef7PSSRARZ6VkQ0P48zkKuZxF/IaREZL2+N8DfCn0XIGviMijInJrpxeKyK0isk9E9s3OzibcrAtD3bdGsuLxB1ZPMW8Rv2Fkia4Rf6+IyHV4wv/G0OY3quoxEdkGfFVEnlLVb0S9XlXvxLeJ9u7dm0qFaY34KynP6lmzegKPP93nYxiGRyIRv4i8Evgz4EZVPRlsV9Vj/u8Z4AvA1Ul83kbFCXniQ4U81bqLairvYYCXww9QsIjfMDLFeQu/iOwGPg/8nKo+Hdo+KiLjwWPgeiAyMygrNCL+vDdzF7xaN2klEP5iXryIP8XnYhjGGl2tHhG5G7gWmBaRo8CHgSKAqt4BfAjYAvyJiADU/Qye7cAX/G0F4FOq+uU+nMOGIYiIg6we8BZcLxXSOV2i7q5ZPfm8RfyGkRV6yeq5ucv+9wLvjdh+CHhV+yuyS+CBBx4/+AuuD61nq86dtYjfsnoMI0ukMxTdoDRF/L7wV1JcqC2wdiyrxzCyhQl/gjRl9eRDEX9KaaSn5i2rxzCyRGLpnEY44s+RE+9xmoU/GJgu+jN3bXDXMLKBCX+CBMKYzwkFb1A71fV6mmfuWq0ew8gKJvwJEkT8gScOpLo0c821PH7DyCLm8SdIbFZPSqn5bS/mgqye9J6LYRhrmPAnSNjjH8qA8AfWTrFgEb9hZAkT/gRpzurxVq1Ks/A3SjbkchTy5vEbRlYw4U+QRsSfD1k9Kfb4LY/fMLKJCX+CtFbnhJRP4GrK489ZOqdhZAQT/gRxWurxQ9qtnrU8/oJF/IaRGUz4E6TmZGzmrrMW8efzNnPXMLLCQAj/bZ9/nI/8/dPdDzxPwlk9a1ZPesWyHhqzsIjfMLLDQAj/V588zteemun754Q9/qEMDO4GbS/mcl7JBhN+w8gEmZ+5u1ytc+JM9YKIVpPHnwmrZy2rxyJ+w8gOmY/4j8ytADC/XOP0Sq2vnxWu1ZPLCcW8pFz412Yi560ev2FkhgEQ/uXIx/0gnMcPUMrnUi38NVcp5gURi/gNI0tkXvgPh8T+cJ+FP+zxA5QKuVR7/HXHpZDz/kW8sszpPRfDMNbIvPAfObXcyLDpt/CHs3rAE/5KLb1iWXO08e3FIn7DyA49Cb+I3CUiMyLyRMx+EZE/EpEDIvK4iLwmtO8WEfmB/3NLUg3vlSNzy1w6PcqW0RLPnrSI/2youy5Ff5A6b7V6DCMz9BrxfwK4ocP+twNX+D+3An8KICKbgQ8DrwWuBj4sIlPn2thz4cjcCjunRti1eeQCePzB4uTZ8PjrjlLIWcRvGFmjp3ROVf2GiFzS4ZAbgf+tqgo8LCKbROQi4Frgq6o6ByAiX8W7gdx9Po3uFVXlyKll3nD5NCOlPI8dme/r57VG/EOFfKoncFWdUMTvZ/WoKuKvLhbmjq8f5KNfO3Chm2gYmWLLWImvf+C6vn9OUnn8O4AjoedH/W1x29sQkVvxvi2we/fuRBp1cqnKctVh1+Yyo0N5/u93n6cWErOkcZx2jz/VVo+jjW8vQeTvKuTbdZ/HDs9TzAv/7jU7L2QTDSNTjA5dmKlVG2YCl6reCdwJsHfv3kQ8hcDa2b15hNGhAo6rPD+/yu4tI0m8fRtBxO9rpCf8Ka/OWcivZfUE2/K5fNuxVcdlx1SZ//6TV17QNhqGcfYkFfoeA3aFnu/0t8VtvyAEWTy7No+we7Mn9s/OLfXt8xzX88QDK2So0H+Pv+64zC1V+/LetRaPH4j1+at1tzFb2TCMjU1SPfVe4Of97J5rgNOq+jxwP3C9iEz5g7rX+9suCEdPebN2d02NcLEf5fczpbPuaiMyBn9wt89Wz6f+5TBv/v0H+nKDqTd5/EHEHy38lbrTSJs1DGNj05PVIyJ34w3UTovIUbxMnSKAqt4B3Ae8AzgALAO/6O+bE5HfAh7x3+r2YKD3QnD45DLTY0OUS3mGCsOUCrm+Cr/juo3IGC5MHv93jpxmcbXO/EqVbePDib533W3O44e1cYxWqnWXyZFSop9vGEZ/6DWr5+Yu+xV4X8y+u4C7zr5p58+RU8vs3lwGIJcTdk2VOdzHXP62iP8CDO4eOnEGgIWVWuLCX3NcisHMXT/yj4/4zeoxjLSQ6Z56eG6ZXZvXBnJ3bx7pr9XjaGMwFPqfx6+qHJr1xiz6UYCudeYudPD4HZehYqb/nQwjM2S2p9Ycl+dPrzYGdcEX/pPLeF9Qkicy4u+j8M8tVRuC3w/hrzvRWT1RVOsuQxbxG0YqyGxPfX5+FcdVdk2FhH/LKIuVOvPL/SnP3OrxDxXyfRX+Z06sZSj1K+Iv9Rrx110b3DWMlJDZnnrk1FoqZ0AQ/ffL7omK+Ct99PgDmwfgdB9uZnW3uTqnty3e6jHhN4x0kNmeeqSRw19ubOu38Ad5/AGB1dMva+ngiTONmbWnV+qJv3+9yeP3/lXiIv5KzQZ3DSMtZLanHp5bppATLpq8cMLfGvEH6+7WYlIgz5dDs0tcsmWU0VK+P1aPG5HHH5fOaRG/YaSGzPbUI6dW2DFVbhLicinP1vGhvqV0Oo42ImNgbd3dPtk9h2bPcOnWUSbLxT4N7vY2c9dxFcdVE37DSAmZ7amH55abMnoCdk2VL6jHD1CpJV+vp+64HJ5bZs/0GBN9Ev5aKD01n4/P6gkGsIcK7TV8DMPYeGRS+Kt1lwPHF9kzPdq2b9NIicVK/7J6ivl24e9HxH/01Ao1RxsR/0JfhN9tq84ZFfEHwm8Rv2Gkg0z21H3PzrFUdXjj5dNt+8qlPMvV/lTMjKrVA/QlpTOYsXtZX62edo8/aryi4nh/TxN+w0gHmeypD+6fpZgX3hAh/CPFPCt9En4vq2ftTxrMZO2L8PupnJdOj/VN+Gtub1k9DavHsnoMIxVksqc+uH+Gq/dsjlzUYGQdIv5uq3B94LPf4SN///RZfdbB2SWmRopMjZb6G/G35fG3n0vFrB7DSBWZ66nH5ld4+vgZrnvptsj95VKhvxH/OXj8Dz9zkju/cYjF1d7F28voGQNgslxkpeYk+s3CdRVX6alWj3n8hpEuMtdTH9w/A8C1L90auX+klKfquNT7MOAal9XTTZBXay7LVYcvfLv3NWoOnVhqDF5PjhSBZMs21BoLx3efubuW1ZO5fyfDyCSZ66kP7p9lx6Yyl/nRcCsjJS/lcLkPKZbttXp6E/4g3fOTDz/b0yzfxdUas4sVLt3qC385EP7kVuKqN9YP9iP+fIeI37GI3zDSRKZ6aqXu8M8HTnDdy7Y2lj9spewLfz/snrrT6vF7n9U14q+7bBsf4unjZ/iXZ7qvUxMUZ7t0es3qgWQj/obw+xF/oYeI30o2GEY6yFRP3ffDUyxXHa59SbS/D6GIvw/C35rV05jA1UH4VZVq3eWnX72DieECn/zW4a6fE2T0XNYW8Scn/EEUH+Tx5xtZPfETuCziN4x0kKme+uD+GUr5HK+/fEvsMeWil+mzXO1DUbM4j9+Jv8kEN4XJkSLv+rFdfPmJ55ldrHT8nEMnlsgJ7PbXEe5LxN/i8Rc61OqxrB7DSBc99VQRuUFE9ovIARH5YMT+PxCRx/yfp0VkPrTPCe27N8nGt/LA/llee+lmRkrxK0qO9NPqiVhzFzpbPcGavMOFPO++Zjc1R/nMviMdP+f5+RW2jQ83SiQ0hD/B0sytHn++Q1ZPpe79LW1w1zDSQdc1d0UkD3wMeCtwFHhERO5V1SeDY1T1V0PH/zLw6tBbrKjqVck1OZrVmsOLJoZ565XbOx7XV6vHia7O2Un4VwPRLOa4bOsYr9w5yT/94ATvu+7y2NccX6ywfWKo8XyiEfEn9y2m5sRE/B09fqvVYxhpoJfF1q8GDqjqIQARuQe4EXgy5vibgQ8n07zeGS7m+eR7X9v1uHIfhb/eksc/XPQ+a7XWQfj9jJ5hP3rfOjbE8cXVjp8zs7DKztDKYsV8LvHSzIHAF/LdI/5gPMDW3DWMdNBLT90BhL2Ho/62NkTkYmAP8LXQ5mER2SciD4vIT8V9iIjc6h+3b3Z2todmnRuBDbRSS97jd1o8/hFf+M9U4j8r8McD0RwbLnBmtXPbZloifiDx2btBxB8MVge/LavHMNJP0j31JuBzqhoOpy9W1b3AfwA+IiKXRb1QVe9U1b2qunfr1ujJV0nQT6un3pLVk8sJI6U8Sx2EvzXiHxsqdLlROMwtVdk+Mdy0PenSzIHH36jO2cjjt6wew0g7vfTUY8Cu0POd/rYobgLuDm9Q1WP+70PAgzT7/xecfubxt0b84An5UocMoqiIf6FDxB9k/Gwbb4/4kyzN3Ij4z2Lmrgm/YaSDXnrqI8AVIrJHREp44t6WnSMiLwOmgG+Gtk2JyJD/eBp4A/FjAxeEwH7pT8TvNnn8EETw8Z/ViPj9do0PFajW3UamTCszvvC3RvzJWz1+xN+6AldMOqcITRlNhmFsXLoO7qpqXUTeD9wP5IG7VPV7InI7sE9Vg5vATcA92lxz4EeAj4uIi3eT+Z1wNtB6UMjnKOVzfZzA1Sx+o0MFznQovhZO5wQYH/YydJYqTuSKVjML3sDvtj57/I08/kIPEb/jLbQeN1vaMIyNRS9ZPajqfcB9Lds+1PL8f0S87iHgFefRvr5QLuVZ6dsEruYvUaNDeZY6RfyhdE7wviEAnFmts3m01Hb88YXA6ulvxN+axy8i5HMSW53TcvgNIz0MZG/tR01+11VU2+2OroO1LRH/2LAn/Asx3xJmFlfJ54QtLTeFpEszt+bxgxf1R0X8lbpLydbbNYzUMJDCXy7lE6/OGQhi6+DuaJfB3daIfzyI+GNuFscXKmwbHyLX8jlJl2ZuzeMH76YWl9VjEb9hpIeB7K0jpTzLHaLwcyGwQKIi/s7pnNEef1wu/8xipS2jB5Kv19Oaxw/xEX/VcS2jxzBSxED21pFiIXGrJxgMjUrnXOyQnllp9fiHO0f8MwurbGvJ6IFw2YaEIv6WPH4IIv4oj9+xyVuGkSIGsreWS3lWErZ64iL+0aEClXr8il9BxB9YJcHgbtwyjMcXVjtG/Enl8rfm8YNXmjne4x/IfyXDSCUD2Vv7Mbjb8PjzrVk9npDHZfZU6g6lwloq5Lgf8S9GRPyVusOp5VpbDj/0wepxm/P4wY/4I/L4qyb8hpEqBrK3eumcFybiHxvy6/XEDPBWai7DIdEcKuQo5iXS459tTN7qv8dfP4usHhvcNYx0MZC91Yv4kx3cDayRqKweIHaAt1J3GrN2wcuXj0sBjcvhh34If0RWTz4mq8cGdw0jVQxkbx0pJT+42ymrB+IHa1drbls547Hh6AHh2cXoWbvgReYjCZZmrrlnF/Hb4K5hpIeB7K3lYp5K3Y3MUDlX4vL4wzNxo1itOY1UzrXXFCOFP4j4ozx+SHb2buvM3eBx3Mxdi/gNIz0MZG9tLL+YYGbPWsQfN7gbZ/W0R/zjQwXOVNoF/PjCKoWcsHmkvZQDJC387daVZfUYRjYYyN66VpM/OZ8/iJBjI/5Yq6c94h8fjvb4ZxYrbI2YtRuQZE3+mqsU89JUeC0u4q/Y4K5hpIqB7K3lYBWuBH3+Tnn8cHYRf5zHfzxm8lZAkjX5a3W37dtLvMdvE7gMI00MZG/txypcwczd1nr8o34651LMZ0V7/NHLL84sRJdrCEjU6vEj/jCxtXocl6GiFWkzjLQwkMLfjwXX4zz+oUKeUj4XW7ahUneb0jnBj/gjrZ7VyBz+gCSFv+a4TRk94Ef8cRO4LOI3jNQwkL01WIUrSasnLqsHgpr88R5/qz8+MVxsW4UrmLUblcMfMFkuslx1GnMKzoe6o23fXrw8fm05zsVVW3bRMNLEQPbWEd/jT3Jw14koYxww2qFCp5fH3271QHMK6MxC/KzdgCQncdXcKI+/Paun6th6u4aRNgayt5b7kM7ZKeLvtBhLpd4e8UdlAgVr7XYb3IVkhL/uRHv89RaPv7HQulk9hpEaBrK39mNwNxj0jFpwvNNiLJVatMcPNI0LNNba7TC4O1H2V+9KQvhdt6kyJ0R7/JW6RfyGkTZ66q0icoOI7BeRAyLywYj9vyAisyLymP/z3tC+W0TkB/7PLUk2/lzpS1ZPTB4/+AuuR1TndF31MmJaRHM8oib/cV/442btgjc2ALDQof5/r1Tr7QvHR+XxBxG/5fEbRnrouti6iOSBjwFvBY4Cj4jIvar6ZMuhn1bV97e8djPwYWAvoMCj/mtPJdL6c6Rh9fTD48+1C+D4UIFjp5bbtgfRcmvEPz7kCXhTxL9Y6ThrFxK2etzorJ5W4beI3zDSRy+99WrggKoeUtUqcA9wY4/v/zbgq6o654v9V4Ebzq2pyVHK58jnJOE8/m5ZPe2fFWTtDEdM4AKayja8sLDacdYurK3ClYjVE+vxW8RvGGmnl966AzgSen7U39bKvxeRx0XkcyKy6yxfi4jcKiL7RGTf7OxsD806d0SEkWKyi7HEzdyF+KyetdW3umf1PD+/yos3lTu2Yc3qSSCrx4ny+HPtVo9l9RhG6kiqt/4dcImqvhIvqv/Ls30DVb1TVfeq6t6tW7cm1Kx4kl6MpVtWz1K1jmqrTRId8Qcef9irPza/wo4uwj9czFHK5xKyes42q8dm7hpGWuhF+I8Bu0LPd/rbGqjqSVWt+E//DPixXl+7XoyU8iyH0jlV9bzKNNed6JIN4EX8rranj8ZF/I1VuPxvCa6rPH96pWvELyJMlAssrJz/2EXdicjjj5jAFdy8LOI3jPTQS299BLhCRPaISAm4Cbg3fICIXBR6+k7g+/7j+4HrRWRKRKaA6/1t6065VGga3P3o1w7wk3/8T+f8fp09/uia/Ku16Ii/sQqXf/yJMxVqjrJjU3xGT8DEcDEhq+fsPH4TfsNID12zelS1LiLvxxPsPHCXqn5PRG4H9qnqvcB/FpF3AnVgDvgF/7VzIvJbeDcPgNtVda4P53HWtC64/p2jp9n/wgKOq5Hi3Y1uWT3gpWduC22v1KMjfvAGeIOI/9j8CkDXiB+8Ad7E8vgjqnO2LrZug7uGkT66Cj+Aqt4H3Ney7UOhx7cBt8W89i7grvNoY18YKeWb0iWfP72Cq3Bqucr0WPwkqTh6ifhbM3viIn4IVuHyBPy5eS+Hv1fhP71cPYuWR1OLqtUTFfHb4K5hpI6B7a3lYvPg7nN+VH3iTKXpuNMrNR54aqbr+3WeuetF9K1lG+Ly+MEb4A1uTMfmvTkAO6a6C/9kuZjIBK7o6pztWT0VK9lgGKljYHurN7jrCeRytc6pZS+6PnmmOVr+zCNH+MVPPMJ8lyi63qFI21jMYixBxB9lk4yH6vs8N7/K+FChka7ZiYnhwrrU6jGrxzDSw8D2Vm9w1xPewEqB9oj/udPeN4G5pc7C7zjxHn/D6qn2HvG3evy92Dyw5vG3po6eLXG1elz1sowCbHDXMNLHwPbW8ODu8764A8wuNgt/UA65WxQdRPxR48JBxN+6GEuniH9saM3qeW5+hRf3kNEDntVTd/W8K4/WHKUYUasHwAndVKxkg2Gkj4HtrSOlPCs1B1Vt+PsAJ1qsnqA4Wjfhd1yvqFl4cfKArlZPpMdfbKRzPje/0pO/D2uzd3u1e56bX4mcvxA5c9e3fpyoiN88fsNIDQPbW8ulPKreJKpj86uIwPTYUJvVE9TB7yXij0sDHSnlEWkX/koHf3x8uEDVcZlfrnJquXYWVk9Qmrn7AO93jszzr3/vAb74+HNN22uOy3LVacwgDggi/nBmT9VxyOek7SZhGMbGpad0ziwSLL+4XK3z3PwK28eHmR4vcTIk/KraiPi75cY7rhuZ0QPehKzRUntp5koXqwdg/wuLAF3LNQQEFTq7TeJSVX7ri0/iuMrRUytN+4Jz3VRuHkzO++MX4Vx+W2/XMNLH4Ap/Y/lFxy+HMMz4cLHJ6llYrTei8vOJ+CF63d1K3avF38ke2n/cE/6eI/7A6lnu3N77vvsC+571qmO3nlvwfFNLCei1iH8ts6dad83fN4yUMbA9dmRobfnF5/zKl61WT7DqFfTo8XeIfEcjll9crTmRGT2wVqjtbCP+iR4i/tWaw29/6fu87EXjbBsfaktVnffPdXKkOeIvRHn8jgm/YaSNge2xwSpcS5V6I13Ss3qqjVTImVCGz3yXCLpbxB+17m6l7kbO2oW1mvxPH18kn5OOSy6GmeyhJv9f/PMPOXpqhf/2b65k82ipMYchIPi20Gr1RHn8FbN6DCN1DKzVUy56p35sfoVq3eXFk8PU/aUQF1bqTI4UG/7+UPF0WtIAABMrSURBVKF7qWPHaV+qMMxYRE3+1ZoTWacH1lbheuqFRV40Mdzz4GnwTeF0zODu6ZUaH3vgAD/xsm288YppNo0U22yh+RXvG8BknMffIvw2ecsw0sXA9tgg4j8wcwagYfUAzPp2TxDxX75tLAGPP8rq6R7xL67We87hByjmc4yU8rFWz8HZM5yp1Hn3NbsB2FQuNYQ+IPh2E+/xtwzumvAbRqoY2B7bSfiDzJ7jC6uMlvJcNFnuweOPz+qBtcVYwlTqHSL+UCplr/5+wGSHCp1z/uB1cK6bRoptNlZwrhMt6ZzBjc1pGdy1iN8w0sXA9thylPCPexFukNkzs1hh+8Qwm0a6lzqu9ZTV074QS2zEP7Qmur1m9ARMDBdjb1RB6YkpP5qfHCky31LiYX65xvhwoc1esojfMLLBwPbYIJ3zmRNLDBdzTI0U2TLqRcFBZs+Mv8D5ZDleSAM8j79LVs9q7xF/sAoXnIPwlwuxVs9JX/i3jHnCv6lcolp3G6uBgRfxt/r7sBbx1x3L6jGMNDOwPTaweip1lxdPlhERNo+WyElI+P2If7JcZKnqUHPc2PfrmtVT8mbiBiUOoHPEH6zCBedq9UQP7s4tVRgu5ho3vk1+ymbY559frja2h4lM57SsHsNIHQPbY72JU97jIKLO5zzxP3Gm0pi1u82P+KFzLr/jupElmQOCwdpwZk+l7kTW6Wl9Ta91egI6Wz21xjcbgKlA+EM+//xKjU3lUttrg6wes3oMI90MbI8VkUbZhnDWzPTYELOLVRYrdVZrbiPih2bhX6k6fPzrBxuLrNfdzumco6HlFwNWa50HRoOUzosme8/qAb80c4zVM7dUYWp0LZqf9AX+VGgS1+mVWtvkLQhV52xK53QoxdhVhmFsTHoSfhG5QUT2i8gBEflgxP5fE5EnReRxEfkHEbk4tM8Rkcf8n3tbX7uelH2746LJtYh6emyIk0uVxqzdbRPREf8D+2f47S891Sh74FXnjP9zjkXU5PcmcHWO+CeGC4z3sABLmIlykTOVelPd/IC5pSqbQxF/YOmEc/lPL9faJm9ByOO3rB7DSDVdJ3CJSB74GPBW4CjwiIjcq6pPhg77NrBXVZdF5JeA3wN+xt+3oqpXJdzuRAh8/rCHPj1W4tnDSxz36/BvGx9uWBlh4X/+tHdjCCZ59ZLHDzQN8FZqTkfR3D4x3HFcIY6J4QKq3hyA1sj95FKVS7eONZ6vefzeuakq8zGDu1ERvw3uGkb66GXm7tXAAVU9BCAi9wA3Ag3hV9UHQsc/DPxsko3sF4Hwh7NmtowNcWKxysyiJ+jbJ4YIZC6c0hkIfrBQi+MqQ6VOEX/7urur9fhaPQC3v/Pl5yb8oXo9rcJ/aqnK5tE1/z7w8gOP/0yljuNq5OBu3ko2GEYm6KXH7gCOhJ4f9bfF8R7gS6HnwyKyT0QeFpGfinuRiNzqH7dvdna2h2adP+VStMe/UnN45oS3wPm2kMcfHgB94Rwj/iCX33GVmqMdI/6p0RLbJs7O3wdiB6NXaw5LVadJ+IeLOUqFXCOrp1GZM2JwtxBTltmsHsNIF4nW6hGRnwX2Am8Obb5YVY+JyKXA10Tku6p6sPW1qnoncCfA3r17z2/B2B4JIv5mj98TvCefO81oKc/YUKEhbGEhfcEX/OOLQcTffeYurGX1VOreDaBTxH+uBKWZWyedBZO3wsIvImwqr9XrCW5uUYO7rRG/qprVYxgppJceewzYFXq+09/WhIi8BfgN4J2q2ihrqarH/N+HgAeBV59HexOlXCywebTUiPwBpv0qmE8+t9CItoP6N6cjrJ5GxO90r84Ja1ZPxZ8wNdwH0YxbjCVK+MGbxRsI/lrE3z2Pv+YoqrbsomGkjV567CPAFSKyR0RKwE1AU3aOiLwa+Die6M+Etk+JyJD/eBp4A6GxgfXmzS/dyk9d1exabfVr2Dx3erWpFHJ49q6qNqyeIPunW1ZPazrnaj1+vd3zJW75xcas3Rbh98o2ePt6i/i9m1bVH38YipmEZhjGxqSr1aOqdRF5P3A/kAfuUtXvicjtwD5VvRf4fWAM+Ky/mtRhVX0n8CPAx0XExbvJ/E5LNtC68nPXXNy2LSheBl5WTUBY+E+v1LxBzUKO4wveZC+ni8dfzOeYGC4w61tDjYi/D6I5EePxn4qJ+DeVixye88Y0ghtAtMffXLLBFlo3jHTSk8evqvcB97Vs+1Do8VtiXvcQ8IrzaeCFJiyKcRF/kMp55UUTPHZknsVKvesELoBdm0caAtuI+Psw+WmsVCAn7VbPWsTfvKjLppEijx9tsXo6RPyB1dMQfpvAZRipwkK1FkqFXMMjb434g8HSYGD3VTsnAc/u6RbxA+zePMKRU77w9zHiz+WE8eH2iqJzSxXyOWkq+Qxe3f1GVs9yjaFCLnLQudBSsmFN+O3fyDDShPXYCILMnm0TzRF/4H8f9yP+V+7c5D1fqFDvUqsHvIj/6KkVXFep1PoX8YPn87daPXNLVaZGSuRablCT5SKrNZfVmsP8cvTkLWivx191vHMw4TeMdGE9NoLA5982Hu3xBxH/K/yI/3iPEf+uqTLVusvsmQqr9f5F/EF7F1rKQM8tVdsGdiFUtmGlxvxKdGVOaK/HXzGP3zBSifXYCIKUzu0tEf9KzaFadzm+sMr0WKlR6uH4QoVal3r8ADs3jwBweG65/xF/pNVTbRvYhebZu/PL0ZU5AfIt6ZyB8NsELsNIF9ZjIwhSOsOzZidDUfELp1fZPjHM6FCB8aFCzxH/bl/4j8wt9z3ijyrNfDJG+NdKM1djK3NCe8RfNeE3jFSS6MzdrPDjL9vG6ZVa0/KH4TIILyxUeLFfKnnrxBAzi6uex99F+INvCEfmVhplIvoV8U9GlGaOi/gnQ4XaTq/UeEVXj98Gdw0jzZjwR/Cml2zlTS/Z2rQtLPzHF1Z5zW5vYHf7+DDHFyreBK4ug7vDxTzbJ4Y4PLfMZn8AuV+TnybKhaYJXHXHZX65Fm31+OvvnvatnrjB3UZWT2sevwm/YaQK67E9Eojh7OIqc0tVXuTbQNsnhji+sOoXaev+59w15aV0Bh5/P2r1gGf1BGMSsFZ2OVhrN0xQnuH4wiorNSd2cDf4QuO0zNw14TeMdGE9tkcC4f/B8TMAbJ8MhH+Y4wurqNLV6gHP5z86t9z3gdHAvgnsnrg6PeAVqyvmhWf9yWWTI9GDuyJCISftefyW1WMYqcJ6bI8Ewr//+CJAI+LfNjFMzbc+ug3ugpfZ8/zCKgurNUT6J5pBhc5ggPfkGV/4I0RdRJgsl3j25BIQXaAtIJ8T8/gNI+VYj+2RoP7N04HwT65ZPQG9RPy7psqowqHZJX/B9+6vORf2TI8C8L3nFoBQxB9h9YCXy//Dk37E30H4wxF/UFrahN8w0oX12B4p5nOMlvIcmvWi4u0Tw02/obeIf5ef0nlw5kzf/H2AH90xyfhwgW8ePAHA3HK81QNeSmdQQC7O44fmiH/NrrJaPYaRJkz4z4LJcpG6q5SLeSb8ejfbQ7N7e/X4AX54conhPgpmPidcc+kWHjp4EoA53+qZivHvJ8vtyzFGUcjn2ssyW8RvGKnCeuxZEAx6XjQ53LBowvV88j349dsnhinmBVf7X8f+9Zdt4dmTyxw9tczcUoWJ4QLFmDaGo/y4CVwQ4/Hb4K5hpArrsWfBpL/ASdjeGS7mG554LxF/PieNiVz9jPgBXn/ZNAAPHTzJyaUqW8aGYo8NBnRzAuND8dM7CjlpyuMv5KSt6JthGBsbE/6zIBD4YGA3IBjg7cXjhzWfv98R/0u2j7FltMQ3D56MnbUbEET8E+ViRyFvjfhtYNcw0of12rMgqk5/+HkvET/AzilP+Psd8YsIr7tsCw8dPNFV+AMbq1MqJzRn9dhC64aRTqzXngWNiH+i2TIJyjf3GvHvvkARP8AbLp/m+EKFAzNnIksyBwSCHzd5K6Apq6fm2sCuYaQQ67VnQTerp1tZ5oBdmz2P/0KkQb7+si2AV1FzqoPwT/Uc8Tdn9VjEbxjpo6deKyI3iMh+ETkgIh+M2D8kIp/2939LRC4J7bvN375fRN6WXNMvPN2snp49/sDquQAR/+7NI43B5I4Rv+/xd5q8Bd63lKdeWOTZk0uex28ZPYaROrr2WhHJAx8D3g5cCdwsIle2HPYe4JSqXg78AfC7/muvBG4CXg7cAPyJ/36p5NW7p3jFjkku3zbWtH0t4j/Lwd0LEPEHPj/ET96CNcHvNHkL4Fff8hJOLVV5+x/+I98+fMoWWjeMFNJLuHY1cEBVD6lqFbgHuLHlmBuBv/Qffw74CfES3W8E7lHViqo+Axzw3y+V/OiOSf7ul9/I+HCzOO6Z9m4EceUQWpkaKTJZLjJRvjBVsV/fg/D3GvFf97JtfPlX3sRVuzbx3OlVs3oMI4X0ojw7gCOh50eB18Ydo6p1ETkNbPG3P9zy2h1RHyIitwK3AuzevbuXtm8YXvqicf7x169j51S5p+NFhE++57Vsn4zPq0+Sd7ziIp4/vco1l26JPWZ8uMgH3vZS3vbyF3V9vxdvKvPJ97yWzz56pLE+sWEY6WHDLMSiqncCdwLs3btX17k5Z01g3/RKsFD7hWC4mOd9113e9bhejgnI5YSf+VfpukEbhuHRy/f0Y8Cu0POd/rbIY0SkAEwCJ3t8rWEYhnEB6UX4HwGuEJE9IlLCG6y9t+WYe4Fb/MfvAr6mqupvv8nP+tkDXAH8SzJNNwzDMM6FrlaP79m/H7gfyAN3qer3ROR2YJ+q3gv8OfBXInIAmMO7OeAf9xngSaAOvE9VnT6di2EYhtED4gXmG4u9e/fqvn371rsZhmEYqUFEHlXVvb0ca7l4hmEYA4YJv2EYxoBhwm8YhjFgmPAbhmEMGBtycFdEZoFnz/Hl08CJBJuTBgbxnGEwz3sQzxkG87zP9pwvVtWtvRy4IYX/fBCRfb2ObGeFQTxnGMzzHsRzhsE8736es1k9hmEYA4YJv2EYxoCRReG/c70bsA4M4jnDYJ73IJ4zDOZ59+2cM+fxG4ZhGJ3JYsRvGIZhdMCE3zAMY8DIjPB3WxA+TYjILhF5QESeFJHvich/8bdvFpGvisgP/N9T/nYRkT/yz/1xEXlN6L1u8Y//gYjcEveZGwkRyYvIt0Xki/7zPSLyLf/8Pu2XB8cv9/1pf/u3ROSS0Hvc5m/fLyJvW58z6Q0R2SQinxORp0Tk+yLyukG41iLyq/7/9xMicreIDGfxWovIXSIyIyJPhLYldn1F5MdE5Lv+a/5IRLov/q2qqf/BKxd9ELgUKAHfAa5c73adx/lcBLzGfzwOPI230P3vAR/0t38Q+F3/8TuALwECXAN8y9++GTjk/57yH0+t9/n1cP6/BnwK+KL//DPATf7jO4Bf8h//J+AO//FNwKf9x1f6/wNDwB7/fyO/3ufV4Xz/Eniv/7gEbMr6tcZbgvUZoBy6xr+QxWsNvAl4DfBEaFti1xdvjZNr/Nd8CXh71zat9x8loT/s64D7Q89vA25b73YleH5/C7wV2A9c5G+7CNjvP/44cHPo+P3+/puBj4e2Nx23EX/wVmn7B+DHgS/6/8wngELrtcZbI+J1/uOCf5y0Xv/wcRvtB2+1umfwEy1ar2FWrzVr63Rv9q/dF4G3ZfVaA5e0CH8i19ff91Roe9NxcT9ZsXqiFoSPXNQ9bfhfaV8NfAvYrqrP+7teALb7j+POP41/l48Avw64/vMtwLyq1v3n4XNonJ+//7R/fJrOew8wC/yFb2/9mYiMkvFrrarHgP8JHAaex7t2j5Ltax0mqeu7w3/cur0jWRH+TCIiY8BfA7+iqgvhferd3jOViysiPwnMqOqj692WC0gBzwb4U1V9NbCE99W/QUav9RRwI96N78XAKHDDujZqnViP65sV4c/cou4iUsQT/f+jqp/3Nx8XkYv8/RcBM/72uPNP29/lDcA7ReSHwD14ds8fAptEJFgmNHwOjfPz908CJ0nXeR8Fjqrqt/znn8O7EWT9Wr8FeEZVZ1W1Bnwe7/pn+VqHSer6HvMft27vSFaEv5cF4VODPyr/58D3VfV/hXaFF7W/Bc/7D7b/vJ8RcA1w2v8aeT9wvYhM+RHW9f62DYmq3qaqO1X1Erxr+DVVfTfwAPAu/7DW8w7+Hu/yj1d/+01+Jsge4Aq8AbANh6q+ABwRkZf6m34Cb43qTF9rPIvnGhEZ8f/fg/PO7LVuIZHr6+9bEJFr/L/jz4feK571HvRIcPDkHXjZLweB31jv9pznubwR76vf48Bj/s878DzNfwB+APw9sNk/XoCP+ef+XWBv6L3+I3DA//nF9T63s/gbXMtaVs+leJ35APBZYMjfPuw/P+DvvzT0+t/w/x776SHLYZ3P9Spgn3+9/wYvayPz1xr4TeAp4Angr/AyczJ3rYG78cYxanjf8N6T5PUF9vp/w4PAR2lJFIj6sZINhmEYA0ZWrB7DMAyjR0z4DcMwBgwTfsMwjAHDhN8wDGPAMOE3DMMYMEz4DcMwBgwTfsMwjAHj/wOqBH8mZYPLPwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "# W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "# W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "# W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "#         z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WRV47WM8qjSp",
        "outputId": "b617f359-1c71-47f7-c97e-f085bf931939"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iters:0\n",
            "Loss:1.264753966728624\n",
            "Pred:[1 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "25 + 98 = 228\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.8626522403690441\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 1 0 1 0 1 1 1]\n",
            "90 + 125 = 181\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9405946009539267\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "48 + 120 = 200\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.6249840058921555\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 1 0 1 0]\n",
            "77 + 29 = 98\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.6216620246224758\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "10 + 111 = 127\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.12536028222931886\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "113 + 57 = 170\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.0805666435658425\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "117 + 7 = 124\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.025660173909972467\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "85 + 77 = 162\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.00857046082469411\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "81 + 58 = 139\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.0038990053116362473\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 0 1 1]\n",
            "88 + 107 = 195\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.0023829427479249822\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "3 + 39 = 42\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.001932441041068653\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "17 + 119 = 136\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.001204066101356704\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "31 + 17 = 48\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.12566222149488257\n",
            "Pred:[0 0 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "30 + 22 = 52\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.0008357541548148074\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "44 + 103 = 147\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.0005829233578012829\n",
            "Pred:[1 1 0 0 1 0 0 1]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "115 + 86 = 201\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.0004683713023023369\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "85 + 54 = 139\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.00041844743376809307\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "117 + 97 = 214\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.0002336924628434216\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "65 + 5 = 70\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.12525489796899988\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "68 + 94 = 162\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.00021623631727477763\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 1 0 1 1 1]\n",
            "44 + 11 = 55\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.00016648179637955562\n",
            "Pred:[0 0 1 1 0 1 0 1]\n",
            "True:[0 0 1 1 0 1 0 1]\n",
            "10 + 43 = 53\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.0002055725477574193\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "59 + 22 = 81\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.0001819365345344023\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "93 + 37 = 130\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.0001927747590001416\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "54 + 117 = 171\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.000122230961838227\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "81 + 16 = 97\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.1250910696662502\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "70 + 18 = 88\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.12509905344559002\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "68 + 26 = 94\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.00011436542223259312\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[0 1 0 1 0 0 1 1]\n",
            "61 + 22 = 83\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.0001092582728172689\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "85 + 99 = 184\n",
            "------------\n",
            "iters:3000\n",
            "Loss:9.359938831513638e-05\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "99 + 12 = 111\n",
            "------------\n",
            "iters:3100\n",
            "Loss:9.440310637081138e-05\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "109 + 16 = 125\n",
            "------------\n",
            "iters:3200\n",
            "Loss:5.392611791535231e-05\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "17 + 74 = 91\n",
            "------------\n",
            "iters:3300\n",
            "Loss:8.080410548765124e-05\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "88 + 97 = 185\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.12505646133248602\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "62 + 62 = 124\n",
            "------------\n",
            "iters:3500\n",
            "Loss:5.328898778281856e-05\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "107 + 27 = 134\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.37502422553159015\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "88 + 24 = 112\n",
            "------------\n",
            "iters:3700\n",
            "Loss:5.5673749309866325e-05\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "31 + 54 = 85\n",
            "------------\n",
            "iters:3800\n",
            "Loss:4.8026762540737866e-05\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "87 + 77 = 164\n",
            "------------\n",
            "iters:3900\n",
            "Loss:3.470845070992636e-05\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "19 + 71 = 90\n",
            "------------\n",
            "iters:4000\n",
            "Loss:4.2700163566354635e-05\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "113 + 93 = 206\n",
            "------------\n",
            "iters:4100\n",
            "Loss:4.795710364473142e-05\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "85 + 62 = 147\n",
            "------------\n",
            "iters:4200\n",
            "Loss:3.565504273486986e-05\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "83 + 59 = 142\n",
            "------------\n",
            "iters:4300\n",
            "Loss:3.278649513036675e-05\n",
            "Pred:[0 0 0 1 1 1 1 1]\n",
            "True:[0 0 0 1 1 1 1 1]\n",
            "14 + 17 = 31\n",
            "------------\n",
            "iters:4400\n",
            "Loss:3.656269211555257e-05\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "126 + 23 = 149\n",
            "------------\n",
            "iters:4500\n",
            "Loss:2.7307650984558785e-05\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "49 + 32 = 81\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.2500167311312978\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "76 + 8 = 84\n",
            "------------\n",
            "iters:4700\n",
            "Loss:2.5781181579605565e-05\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "57 + 113 = 170\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.1250249773124422\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "124 + 54 = 178\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.12502113692271843\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "72 + 102 = 174\n",
            "------------\n",
            "iters:5000\n",
            "Loss:1.511011254483561e-05\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "72 + 5 = 77\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.9510143793060176e-05\n",
            "Pred:[0 0 0 1 0 1 1 1]\n",
            "True:[0 0 0 1 0 1 1 1]\n",
            "10 + 13 = 23\n",
            "------------\n",
            "iters:5200\n",
            "Loss:1.3801709749044701e-05\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "31 + 19 = 50\n",
            "------------\n",
            "iters:5300\n",
            "Loss:2.4521257753528154e-05\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "108 + 27 = 135\n",
            "------------\n",
            "iters:5400\n",
            "Loss:1.9497580055157186e-05\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "54 + 21 = 75\n",
            "------------\n",
            "iters:5500\n",
            "Loss:1.406897762512383e-05\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "57 + 43 = 100\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.12501520440869132\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "34 + 100 = 134\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.12501423576173282\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "10 + 110 = 120\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.12501641505148522\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "110 + 22 = 132\n",
            "------------\n",
            "iters:5900\n",
            "Loss:1.1953832760834829e-05\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "83 + 66 = 149\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.12501525876368227\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "54 + 90 = 144\n",
            "------------\n",
            "iters:6100\n",
            "Loss:1.4904918117324281e-05\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "35 + 24 = 59\n",
            "------------\n",
            "iters:6200\n",
            "Loss:1.7156659326860862e-05\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "87 + 14 = 101\n",
            "------------\n",
            "iters:6300\n",
            "Loss:1.2585972879920099e-05\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "14 + 91 = 105\n",
            "------------\n",
            "iters:6400\n",
            "Loss:1.608157506391354e-05\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "55 + 10 = 65\n",
            "------------\n",
            "iters:6500\n",
            "Loss:1.4636636643948624e-05\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "88 + 97 = 185\n",
            "------------\n",
            "iters:6600\n",
            "Loss:7.298786966714602e-06\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "73 + 5 = 78\n",
            "------------\n",
            "iters:6700\n",
            "Loss:1.4944572900751452e-05\n",
            "Pred:[1 1 1 0 1 0 0 1]\n",
            "True:[1 1 1 0 1 0 0 1]\n",
            "107 + 126 = 233\n",
            "------------\n",
            "iters:6800\n",
            "Loss:1.295971995153531e-05\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "59 + 74 = 133\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.1250079711532116\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "26 + 86 = 112\n",
            "------------\n",
            "iters:7000\n",
            "Loss:1.4093555892288302e-05\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "20 + 105 = 125\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.25000786558002863\n",
            "Pred:[0 0 1 0 1 1 0 0]\n",
            "True:[0 0 1 0 1 1 0 0]\n",
            "44 + 0 = 44\n",
            "------------\n",
            "iters:7200\n",
            "Loss:9.132103651303259e-06\n",
            "Pred:[1 1 0 1 0 1 0 1]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "122 + 91 = 213\n",
            "------------\n",
            "iters:7300\n",
            "Loss:9.745768836822768e-06\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "21 + 101 = 122\n",
            "------------\n",
            "iters:7400\n",
            "Loss:8.704433103663973e-06\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "77 + 31 = 108\n",
            "------------\n",
            "iters:7500\n",
            "Loss:1.0439536646420528e-05\n",
            "Pred:[1 0 1 0 0 0 1 1]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "76 + 87 = 163\n",
            "------------\n",
            "iters:7600\n",
            "Loss:8.644949839239184e-06\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "101 + 41 = 142\n",
            "------------\n",
            "iters:7700\n",
            "Loss:8.090668278102876e-06\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "117 + 82 = 199\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.12500867958314496\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "46 + 28 = 74\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.1250062784814747\n",
            "Pred:[1 0 1 0 1 0 1 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "66 + 104 = 170\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.2500067551492168\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "36 + 72 = 108\n",
            "------------\n",
            "iters:8100\n",
            "Loss:7.74591522096896e-06\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "119 + 3 = 122\n",
            "------------\n",
            "iters:8200\n",
            "Loss:4.157099240666327e-06\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "37 + 33 = 70\n",
            "------------\n",
            "iters:8300\n",
            "Loss:5.255243514867441e-06\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "67 + 41 = 108\n",
            "------------\n",
            "iters:8400\n",
            "Loss:6.987332975578635e-06\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "23 + 103 = 126\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.1250060333436514\n",
            "Pred:[0 1 0 1 0 1 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "54 + 30 = 84\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.12500497578351055\n",
            "Pred:[0 1 1 0 1 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "22 + 82 = 104\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.1250064396207748\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "24 + 118 = 142\n",
            "------------\n",
            "iters:8800\n",
            "Loss:4.440815766251544e-06\n",
            "Pred:[0 0 1 0 0 0 1 1]\n",
            "True:[0 0 1 0 0 0 1 1]\n",
            "1 + 34 = 35\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.125005527644689\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "50 + 44 = 94\n",
            "------------\n",
            "iters:9000\n",
            "Loss:4.53755450528495e-06\n",
            "Pred:[0 1 0 1 1 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "8 + 81 = 89\n",
            "------------\n",
            "iters:9100\n",
            "Loss:4.999046637031403e-06\n",
            "Pred:[0 0 1 1 0 0 0 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "1 + 48 = 49\n",
            "------------\n",
            "iters:9200\n",
            "Loss:4.433861891311336e-06\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "53 + 32 = 85\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.12500469097001016\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "92 + 18 = 110\n",
            "------------\n",
            "iters:9400\n",
            "Loss:6.751348546532371e-06\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "111 + 47 = 158\n",
            "------------\n",
            "iters:9500\n",
            "Loss:3.7570334333842877e-06\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "100 + 65 = 165\n",
            "------------\n",
            "iters:9600\n",
            "Loss:5.550946557700648e-06\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "111 + 49 = 160\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.125005362478846\n",
            "Pred:[0 1 0 1 1 1 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "44 + 50 = 94\n",
            "------------\n",
            "iters:9800\n",
            "Loss:5.630057582907894e-06\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "81 + 45 = 126\n",
            "------------\n",
            "iters:9900\n",
            "Loss:4.1981033721135656e-06\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "55 + 101 = 156\n",
            "------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 1\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "# W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "# W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "# W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "# Xavier\n",
        "# W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "# W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "#         z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "#         z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        z[:,t+1] = np.tanh(u[:,t+1])    \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "#         delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])    \n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zqg2RJIZrHcS",
        "outputId": "93dfd128-8214-4e1a-89b8-79edf8ade590"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iters:0\n",
            "Loss:1.066049494124882\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "72 + 124 = 128\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.6388461031803989\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "55 + 23 = 110\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.7008834839039786\n",
            "Pred:[0 0 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "84 + 7 = 27\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.7357716365595818\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "71 + 80 = 135\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.6822604756567112\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "17 + 101 = 22\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.8966560358346877\n",
            "Pred:[0 0 1 0 1 0 1 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "15 + 34 = 43\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.5770958826294741\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "20 + 39 = 91\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.40483268763541314\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "79 + 49 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.4557438774944433\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "58 + 22 = 48\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.09177740452852708\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "18 + 51 = 69\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.044203088761843846\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "49 + 27 = 76\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.07015244460422052\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[0 0 0 1 0 1 1 0]\n",
            "17 + 5 = 22\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.013350589379994813\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "38 + 127 = 165\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.06909148532925445\n",
            "Pred:[1 1 1 0 1 0 0 1]\n",
            "True:[1 1 1 0 1 0 0 1]\n",
            "119 + 114 = 233\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.006751368166069144\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "5 + 71 = 76\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.8408402228759537\n",
            "Pred:[0 0 0 1 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "55 + 46 = 21\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.025568687824244984\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "78 + 73 = 151\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.16576603898071765\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "54 + 28 = 82\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.5060446310935656\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "41 + 119 = 128\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.011844906879914342\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "24 + 53 = 77\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.2530836327347268\n",
            "Pred:[1 1 1 1 0 1 0 0]\n",
            "True:[1 1 1 1 0 1 0 0]\n",
            "124 + 120 = 244\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.004034004204665603\n",
            "Pred:[1 0 1 0 0 1 0 1]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "59 + 106 = 165\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.0026114327558853434\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "56 + 39 = 95\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.001991095684557977\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "83 + 73 = 156\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.0017658308869588843\n",
            "Pred:[1 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "111 + 83 = 194\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.0012454866199517391\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "59 + 102 = 161\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.2507710766883196\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "120 + 36 = 156\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.0006096316207576763\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "99 + 1 = 100\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.0006607254213339798\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "31 + 76 = 107\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.0006090248730288964\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "42 + 75 = 117\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.12540997791894778\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "114 + 58 = 172\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.00022210683907985954\n",
            "Pred:[0 0 1 1 0 0 0 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "32 + 17 = 49\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.00023418180261592167\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "116 + 83 = 199\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.0002923644748500135\n",
            "Pred:[1 0 1 1 1 1 0 1]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "126 + 63 = 189\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.12527740723638234\n",
            "Pred:[1 0 1 0 1 1 1 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "66 + 108 = 174\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.0001414481087120879\n",
            "Pred:[1 1 1 0 0 1 0 1]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "113 + 116 = 229\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.00033192928241508265\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "47 + 40 = 87\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.00023897417873226215\n",
            "Pred:[1 1 0 0 0 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "105 + 94 = 199\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.0002564753408499828\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "15 + 120 = 135\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.1250853350270528\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "56 + 46 = 102\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.0001478966337261453\n",
            "Pred:[1 0 0 1 1 0 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "71 + 82 = 153\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.0001353982004521521\n",
            "Pred:[1 0 0 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "23 + 117 = 140\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.00017419988458825596\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "55 + 64 = 119\n",
            "------------\n",
            "iters:4300\n",
            "Loss:8.358224320683455e-05\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "7 + 102 = 109\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.00014115414854682588\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "55 + 105 = 160\n",
            "------------\n",
            "iters:4500\n",
            "Loss:7.440091211177466e-05\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "72 + 43 = 115\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.00011217437854243351\n",
            "Pred:[1 0 0 1 0 1 1 1]\n",
            "True:[1 0 0 1 0 1 1 1]\n",
            "98 + 53 = 151\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.00011755528082770864\n",
            "Pred:[1 0 1 1 1 0 1 0]\n",
            "True:[1 0 1 1 1 0 1 0]\n",
            "65 + 121 = 186\n",
            "------------\n",
            "iters:4800\n",
            "Loss:8.041352985532352e-05\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "125 + 47 = 172\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.37504059409338353\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "120 + 56 = 176\n",
            "------------\n",
            "iters:5000\n",
            "Loss:7.618975800579643e-05\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "78 + 107 = 185\n",
            "------------\n",
            "iters:5100\n",
            "Loss:7.0410952794221e-05\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "1 + 116 = 117\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.00010159040360125891\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "15 + 120 = 135\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.12504159997206513\n",
            "Pred:[0 0 0 1 1 1 0 0]\n",
            "True:[0 0 0 1 1 1 0 0]\n",
            "18 + 10 = 28\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.1250416156325908\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "50 + 10 = 60\n",
            "------------\n",
            "iters:5500\n",
            "Loss:6.245781831725604e-05\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "20 + 117 = 137\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.12504743311154912\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "4 + 50 = 54\n",
            "------------\n",
            "iters:5700\n",
            "Loss:5.4414869292624216e-05\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "89 + 7 = 96\n",
            "------------\n",
            "iters:5800\n",
            "Loss:5.0441615848925527e-05\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "10 + 59 = 69\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.1250440040148482\n",
            "Pred:[0 1 0 1 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "36 + 50 = 86\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.12503142279362334\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "96 + 70 = 166\n",
            "------------\n",
            "iters:6100\n",
            "Loss:3.5035112643012484e-05\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "75 + 45 = 120\n",
            "------------\n",
            "iters:6200\n",
            "Loss:2.5291052905493485e-05\n",
            "Pred:[0 0 0 1 0 1 1 1]\n",
            "True:[0 0 0 1 0 1 1 1]\n",
            "12 + 11 = 23\n",
            "------------\n",
            "iters:6300\n",
            "Loss:2.97609638575799e-05\n",
            "Pred:[0 0 1 0 0 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "24 + 15 = 39\n",
            "------------\n",
            "iters:6400\n",
            "Loss:4.276116726704172e-05\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "39 + 74 = 113\n",
            "------------\n",
            "iters:6500\n",
            "Loss:3.696208731841305e-05\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "108 + 37 = 145\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.12503251295132628\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "70 + 110 = 180\n",
            "------------\n",
            "iters:6700\n",
            "Loss:1.8000397161405648e-05\n",
            "Pred:[0 1 0 0 0 1 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "64 + 5 = 69\n",
            "------------\n",
            "iters:6800\n",
            "Loss:4.7936660094258506e-05\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "121 + 22 = 143\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.12503302923774515\n",
            "Pred:[0 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "78 + 0 = 78\n",
            "------------\n",
            "iters:7000\n",
            "Loss:2.616496983632759e-05\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "19 + 107 = 126\n",
            "------------\n",
            "iters:7100\n",
            "Loss:2.8357080442559264e-05\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "73 + 75 = 148\n",
            "------------\n",
            "iters:7200\n",
            "Loss:2.7256068795666305e-05\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "85 + 125 = 210\n",
            "------------\n",
            "iters:7300\n",
            "Loss:3.23570468572996e-05\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "21 + 105 = 126\n",
            "------------\n",
            "iters:7400\n",
            "Loss:2.2142389238152142e-05\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "82 + 49 = 131\n",
            "------------\n",
            "iters:7500\n",
            "Loss:1.8778234592474263e-05\n",
            "Pred:[1 1 1 0 1 0 1 1]\n",
            "True:[1 1 1 0 1 0 1 1]\n",
            "110 + 125 = 235\n",
            "------------\n",
            "iters:7600\n",
            "Loss:1.8508090883675632e-05\n",
            "Pred:[0 1 0 0 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 0]\n",
            "19 + 49 = 68\n",
            "------------\n",
            "iters:7700\n",
            "Loss:2.1671308776364764e-05\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "93 + 34 = 127\n",
            "------------\n",
            "iters:7800\n",
            "Loss:2.2986029050655317e-05\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "74 + 57 = 131\n",
            "------------\n",
            "iters:7900\n",
            "Loss:1.8891658373986126e-05\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "65 + 31 = 96\n",
            "------------\n",
            "iters:8000\n",
            "Loss:2.82810226202374e-05\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "123 + 4 = 127\n",
            "------------\n",
            "iters:8100\n",
            "Loss:2.328432341209422e-05\n",
            "Pred:[1 0 1 0 1 0 0 1]\n",
            "True:[1 0 1 0 1 0 0 1]\n",
            "51 + 118 = 169\n",
            "------------\n",
            "iters:8200\n",
            "Loss:1.190897245040756e-05\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "1 + 96 = 97\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.12501792323415625\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "112 + 66 = 178\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.12501254517486785\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "6 + 76 = 82\n",
            "------------\n",
            "iters:8500\n",
            "Loss:1.0846645941577662e-05\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "125 + 95 = 220\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.3750146497846022\n",
            "Pred:[1 0 1 0 1 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "104 + 64 = 168\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.1250122197917147\n",
            "Pred:[1 1 1 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "126 + 100 = 226\n",
            "------------\n",
            "iters:8800\n",
            "Loss:2.0105826854225996e-05\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "60 + 75 = 135\n",
            "------------\n",
            "iters:8900\n",
            "Loss:1.7229577338061196e-05\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "111 + 50 = 161\n",
            "------------\n",
            "iters:9000\n",
            "Loss:1.5769796869127905e-05\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "73 + 36 = 109\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.12501109202012756\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 0 0 0]\n",
            "58 + 22 = 80\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.125011676862664\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "42 + 118 = 160\n",
            "------------\n",
            "iters:9300\n",
            "Loss:1.3055078657909838e-05\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "26 + 37 = 63\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.2500098562647638\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "112 + 44 = 156\n",
            "------------\n",
            "iters:9500\n",
            "Loss:1.6246254183683812e-05\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "31 + 106 = 137\n",
            "------------\n",
            "iters:9600\n",
            "Loss:1.817673758862372e-05\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "23 + 80 = 103\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.12501066319461662\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "38 + 76 = 114\n",
            "------------\n",
            "iters:9800\n",
            "Loss:1.0772795944934532e-05\n",
            "Pred:[0 1 1 0 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "10 + 95 = 105\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.12501328311600862\n",
            "Pred:[1 0 1 0 1 0 0 0]\n",
            "True:[1 0 1 0 1 0 0 0]\n",
            "78 + 90 = 168\n",
            "------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBt7vxmO-bn"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ]
}